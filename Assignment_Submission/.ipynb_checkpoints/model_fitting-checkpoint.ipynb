{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import matplotlib as plt\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version 0.23.4\n",
      "numpy version 1.15.4\n",
      "xgboost version 0.80\n",
      "matplotlib version 3.0.2\n",
      "re version 2.2.1\n"
     ]
    }
   ],
   "source": [
    "print('pandas version '+pd.__version__)\n",
    "print('numpy version '+np.__version__)\n",
    "print('xgboost version '+xgb.__version__)\n",
    "print('matplotlib version '+plt.__version__)\n",
    "print('re version '+re.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_NOTEBOOK = time.time()\n",
    "#Bool indicating whether or not we are loading the serialised model using pickle\n",
    "LOAD_MODEL=True\n",
    "#Bool indicating whether or not we are outputting files for submission\n",
    "WRITE_FILE=True\n",
    "#String indicating whether we are validating the model or training it for final submission, \n",
    "#can take values of 'VAL' or 'TEST' \n",
    "PHASE='TEST'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load in the data prepared previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train , x_test , y_train , y_test = pickle.load(open('saved_datasets/19-3-19,pipeline_A_reduced_frommaster,dateback=18/xtrain_xtest_ytrain_ytest.pickle','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clipping the target and droping columns used for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_model3(x_train,x_test,y_train,y_test):\n",
    "\n",
    "    x_train = [d.drop(['shop_id','item_id','item_category_id','city','item_type','item_info','month'],axis=1) for d in x_train]\n",
    "    x_test =  [d.drop(['shop_id','item_id','item_category_id','city','item_type','item_info','month'],axis=1) for d in x_test]\n",
    "    y_train = [d.clip(0,20) for d in y_train]\n",
    "    y_test =  [d.clip(0,20) for d in y_test]\n",
    "    return [x_train,x_test,y_train,y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train , x_test , y_train , y_test = preproc_model3(x_train,x_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Preprocessing Attempts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempts were made to better process the features, these included averaging over a longer period, dropping early data and non-negative matrix factorisations. Ultimately, none of these proved useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try averaging some features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A descision was made not to use this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try culling early data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps there is data so old, that we shouldn't use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some experimentation it was decided that all features generated would be used, **this was not the case originally** but the data preparation notebook was edited to produce only the useful features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date_block_num',\n",
       " 'sum_item_sales_back_1',\n",
       " 'sum_shop_sales_back_1',\n",
       " 'item_cnt_month_back_1',\n",
       " 'sum_item_cat_sales_back_1',\n",
       " 'sum_city_back_1',\n",
       " 'sum_itemtype_back_1',\n",
       " 'sum_iteminfo_back_1',\n",
       " 'sum_item_sales_back_2',\n",
       " 'sum_shop_sales_back_2',\n",
       " 'item_cnt_month_back_2',\n",
       " 'sum_item_cat_sales_back_2',\n",
       " 'sum_city_back_2',\n",
       " 'sum_itemtype_back_2',\n",
       " 'sum_iteminfo_back_2',\n",
       " 'sum_item_sales_back_3',\n",
       " 'sum_shop_sales_back_3',\n",
       " 'item_cnt_month_back_3',\n",
       " 'sum_item_cat_sales_back_3',\n",
       " 'sum_city_back_3',\n",
       " 'sum_itemtype_back_3',\n",
       " 'sum_iteminfo_back_3',\n",
       " 'sum_item_sales_back_4',\n",
       " 'sum_shop_sales_back_4',\n",
       " 'item_cnt_month_back_4',\n",
       " 'sum_item_cat_sales_back_4',\n",
       " 'sum_city_back_4',\n",
       " 'sum_itemtype_back_4',\n",
       " 'sum_iteminfo_back_4',\n",
       " 'sum_item_sales_back_5',\n",
       " 'sum_shop_sales_back_5',\n",
       " 'item_cnt_month_back_5',\n",
       " 'sum_item_cat_sales_back_5',\n",
       " 'sum_city_back_5',\n",
       " 'sum_itemtype_back_5',\n",
       " 'sum_iteminfo_back_5',\n",
       " 'sum_item_sales_back_6',\n",
       " 'sum_shop_sales_back_6',\n",
       " 'item_cnt_month_back_6',\n",
       " 'sum_item_cat_sales_back_6',\n",
       " 'sum_city_back_6',\n",
       " 'sum_itemtype_back_6',\n",
       " 'sum_iteminfo_back_6',\n",
       " 'sum_item_sales_back_7',\n",
       " 'sum_shop_sales_back_7',\n",
       " 'item_cnt_month_back_7',\n",
       " 'sum_item_cat_sales_back_7',\n",
       " 'sum_city_back_7',\n",
       " 'sum_itemtype_back_7',\n",
       " 'sum_iteminfo_back_7',\n",
       " 'sum_item_sales_back_8',\n",
       " 'sum_shop_sales_back_8',\n",
       " 'item_cnt_month_back_8',\n",
       " 'sum_item_cat_sales_back_8',\n",
       " 'sum_city_back_8',\n",
       " 'sum_itemtype_back_8',\n",
       " 'sum_iteminfo_back_8',\n",
       " 'sum_item_sales_back_9',\n",
       " 'sum_shop_sales_back_9',\n",
       " 'item_cnt_month_back_9',\n",
       " 'sum_item_cat_sales_back_9',\n",
       " 'sum_city_back_9',\n",
       " 'sum_itemtype_back_9',\n",
       " 'sum_iteminfo_back_9',\n",
       " 'sum_item_sales_back_10',\n",
       " 'sum_shop_sales_back_10',\n",
       " 'item_cnt_month_back_10',\n",
       " 'sum_item_cat_sales_back_10',\n",
       " 'sum_city_back_10',\n",
       " 'sum_itemtype_back_10',\n",
       " 'sum_iteminfo_back_10',\n",
       " 'sum_item_sales_back_11',\n",
       " 'sum_shop_sales_back_11',\n",
       " 'item_cnt_month_back_11',\n",
       " 'sum_item_cat_sales_back_11',\n",
       " 'sum_city_back_11',\n",
       " 'sum_itemtype_back_11',\n",
       " 'sum_iteminfo_back_11',\n",
       " 'sum_item_sales_back_12',\n",
       " 'sum_shop_sales_back_12',\n",
       " 'item_cnt_month_back_12',\n",
       " 'sum_item_cat_sales_back_12',\n",
       " 'sum_city_back_12',\n",
       " 'sum_itemtype_back_12',\n",
       " 'sum_iteminfo_back_12',\n",
       " 'sum_item_sales_back_13',\n",
       " 'sum_shop_sales_back_13',\n",
       " 'item_cnt_month_back_13',\n",
       " 'sum_item_cat_sales_back_13',\n",
       " 'sum_city_back_13',\n",
       " 'sum_itemtype_back_13',\n",
       " 'sum_iteminfo_back_13',\n",
       " 'sum_item_sales_back_14',\n",
       " 'sum_shop_sales_back_14',\n",
       " 'item_cnt_month_back_14',\n",
       " 'sum_item_cat_sales_back_14',\n",
       " 'sum_city_back_14',\n",
       " 'sum_itemtype_back_14',\n",
       " 'sum_iteminfo_back_14',\n",
       " 'sum_item_sales_back_15',\n",
       " 'sum_shop_sales_back_15',\n",
       " 'item_cnt_month_back_15',\n",
       " 'sum_item_cat_sales_back_15',\n",
       " 'sum_city_back_15',\n",
       " 'sum_itemtype_back_15',\n",
       " 'sum_iteminfo_back_15',\n",
       " 'sum_item_sales_back_16',\n",
       " 'sum_shop_sales_back_16',\n",
       " 'item_cnt_month_back_16',\n",
       " 'sum_item_cat_sales_back_16',\n",
       " 'sum_city_back_16',\n",
       " 'sum_itemtype_back_16',\n",
       " 'sum_iteminfo_back_16',\n",
       " 'sum_item_sales_back_17',\n",
       " 'sum_shop_sales_back_17',\n",
       " 'item_cnt_month_back_17',\n",
       " 'sum_item_cat_sales_back_17',\n",
       " 'sum_city_back_17',\n",
       " 'sum_itemtype_back_17',\n",
       " 'sum_iteminfo_back_17',\n",
       " 'sum_item_sales_back_18',\n",
       " 'sum_shop_sales_back_18',\n",
       " 'item_cnt_month_back_18',\n",
       " 'sum_item_cat_sales_back_18',\n",
       " 'sum_city_back_18',\n",
       " 'sum_itemtype_back_18',\n",
       " 'sum_iteminfo_back_18',\n",
       " 'first_sale_date_block',\n",
       " 'new_item',\n",
       " 'numdays',\n",
       " 'month_sales',\n",
       " 'mean_prevmonth_item_price',\n",
       " 'std_prevmonth_item_price',\n",
       " 'median_prevmonth_item_price',\n",
       " 'median_prevmonth_shop_item_price',\n",
       " 'prop_median_item_price']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Validation/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PHASE=='VAL':\n",
    "    xtrain = x_train[0]\n",
    "    ytrain = y_train[0]\n",
    "    xtest = x_test[0]\n",
    "    ytest = y_test[0]\n",
    "    eval_set = [(xtrain,ytrain),(xtest,ytest)]\n",
    "    n_estimators = 50000\n",
    "elif PHASE=='TEST':\n",
    "    xtrain = x_train[1]\n",
    "    ytrain = y_train[1]\n",
    "    xtest = x_test[1]\n",
    "    ytest = y_test[1]\n",
    "    eval_set = [(xtrain,ytrain)]\n",
    "    n_estimators = 1943"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBRegressor(\n",
    "    seed=0,\n",
    "    max_depth=2,\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=n_estimators,\n",
    "    objective='reg:linear',\n",
    "    nthread=7,\n",
    "    colsample_bytree=1,\n",
    "    subsample=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_MODEL:\n",
    "    model = pickle.load(open('model.pickle','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_MODEL:\n",
    "    model.fit(\n",
    "        verbose=True,\n",
    "        X=xtrain,\n",
    "        y=ytrain,\n",
    "        eval_set=eval_set,\n",
    "        early_stopping_rounds=500\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_MODEL:\n",
    "    pickle.dump(model,open('model.pickle','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of plots are used to better understand both model and feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy per tree in forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PHASE=='VAL':\n",
    "    deval = model.evals_result()\n",
    "    val0 = deval['validation_0']['rmse']\n",
    "    val1 = deval['validation_1']['rmse']\n",
    "    deval = pd.DataFrame({'val0':val0,'val1':val1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a plot of rmse as a function of the number of trees used, `val0` is the training set and `val1` the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PHASE=='VAL':\n",
    "    deval.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the frequency metric, which only counts the number of times a tree is split on each variable, gain measures the increase in accuracy contributed by each feature. After splitting, the loss function value decreases, this decrease corresponds to an increase in the gain value for the feature that was splitted on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gain = pd.Series(model.get_booster().get_score(importance_type='gain'))\n",
    "gain = gain.sort_values(ascending=True)\n",
    "type(gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = gain.plot(kind='barh',color='b')\n",
    "ax.figure.set_size_inches(10,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gain by time-indexed variable type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some variables are lagged over time. Here their effects are aggregated to give a better overall picture of time-based variable importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d = pd.DataFrame()\n",
    "d = {}\n",
    "coltypes=[  \n",
    " 'sum_item_sales_back_',\n",
    " 'sum_shop_sales_back_',\n",
    " 'item_cnt_month_back_',\n",
    " 'sum_item_cat_sales_back_',\n",
    " 'sum_item_cat_shop_sales_back_',\n",
    " 'sum_city_back_',\n",
    " 'sum_itemtype_back_',\n",
    " 'sum_iteminfo_back_',\n",
    " 'sum_city_item_back_',\n",
    " 'sum_city_item_cat_back_',\n",
    " 'sum_itemtype_shop_back_',\n",
    " 'sum_itemtype_city_back_']\n",
    "save=[]\n",
    "for ct in coltypes:\n",
    "    save.append(gain.filter(regex=ct))\n",
    "    d[ct] = np.sum(gain.filter(regex=ct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('item_cnt_month_back_', 103314.54555532697),\n",
       " ('sum_shop_sales_back_', 21380.465809077472),\n",
       " ('sum_city_back_', 20044.459942743037),\n",
       " ('sum_item_cat_sales_back_', 12652.016951253481),\n",
       " ('sum_itemtype_back_', 12535.807207940754),\n",
       " ('sum_item_sales_back_', 7747.190809840945),\n",
       " ('sum_iteminfo_back_', 5462.923161475649),\n",
       " ('sum_item_cat_shop_sales_back_', 0.0),\n",
       " ('sum_city_item_back_', 0.0),\n",
       " ('sum_city_item_cat_back_', 0.0),\n",
       " ('sum_itemtype_shop_back_', 0.0),\n",
       " ('sum_itemtype_city_back_', 0.0)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(d.items(),key=lambda t: t[1],reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, `item_cnt_month_back_` is the most important of these features, it is the target variable that has been lagged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if WRITE_FILE:\n",
    "    submission = pd.DataFrame({'ID':np.arange(0,214200),'item_cnt_month':preds})\n",
    "    submission.to_csv('submissions/submission'+str(datetime.datetime.now()).replace(':','.')+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission 2, Adjusting for Leak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if WRITE_FILE:\n",
    "    submission['item_cnt_month'] = 0.2839365/submission.item_cnt_month.mean() * submission.item_cnt_month\n",
    "    submission.to_csv('submissions/submission_adjusted_'+str(datetime.datetime.now()).replace(':','.')+'.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
