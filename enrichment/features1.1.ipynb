{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from itertools import product\n",
    "import gc\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATEBACK_DIST=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to downcast data types to 32 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downcast(df):\n",
    "    float_cols = [col for col in df if df[col].dtype=='float64']\n",
    "    int_cols = [col for col in df if df[col].dtype=='int64']\n",
    "\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols] = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_list = pickle.load(open('../gen_data/x_train--simple_validation_split.ipynb--.pickle','rb'))\n",
    "y_train_list = pickle.load(open('../gen_data/y_train--simple_validation_split.ipynb--.pickle','rb'))\n",
    "x_test_list = pickle.load(open('../gen_data/test_data_enriched--enrich1.ipynb--.pickle','rb'))\n",
    "\n",
    "items_data = pd.read_csv('../original_data/items.csv')\n",
    "sales = pd.read_csv('../gen_data/train1.csv')\n",
    "df_item_cat = pd.read_csv('../gen_data/train2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_list = list(map(downcast,x_train_list))\n",
    "x_test_list = list(map(downcast,x_test_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate new item variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with sales data, figure out when each item was sold, \n",
    "    #merge this with train/test then transform it to 1/0\n",
    "def new_item_var(df):\n",
    "    sales2 = sales.copy()\n",
    "    first_sale_date = sales2.groupby('item_id').date_block_num.min()\n",
    "    sales2['first_sale_date_block'] = sales2.item_id.map(first_sale_date)\n",
    "    sales2 = sales2[['item_id','first_sale_date_block']]\n",
    "    sales2 = sales2.drop_duplicates()\n",
    "    df = df.merge(sales2,on='item_id',how='left')\n",
    "    \n",
    "    #handle nans for test data (which sales2 wont pick up)\n",
    "    df.first_sale_date_block.fillna(999,inplace=True)    \n",
    "    df['new_item'] = (df.first_sale_date_block >= df.date_block_num)*1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_list = list(map(new_item_var,x_train_list))\n",
    "x_test_list = list(map(new_item_var,x_test_list))\n",
    "del sales\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse City data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city = df_item_cat.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_series = pd.Series([re.search('(.*?) ',n).group() for n in df_city.shop_name],index=df_city.index)\n",
    "df_city['city'] = cities_series\n",
    "df_city = df_city[['shop_id','city']].drop_duplicates()\n",
    "df_city = downcast(df_city)\n",
    "\n",
    "x_train_list = list(map(lambda x: x.merge(df_city,on=['shop_id'],how='left'),x_train_list))\n",
    "x_test_list = list(map(lambda x: x.merge(df_city,on=['shop_id'],how='left'),x_test_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse item category data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_type_series = df_item_cat.item_category_name.map(lambda x: re.split('-',x)[0])\n",
    "df_item_cat['item_type'] = item_type_series\n",
    "\n",
    "item_info_series = df_item_cat.item_category_name.map(lambda x: '-'.join(re.split('-',x)[1:]) if len(re.split('-',x))>1 else 'NAN')\n",
    "df_item_cat['item_info'] = item_info_series\n",
    "\n",
    "df_item_cat = df_item_cat[['item_category_id','item_type','item_info']].drop_duplicates()\n",
    "df_item_cat = downcast(df_item_cat)\n",
    "\n",
    "x_train_list = list(map(lambda x: x.merge(df_item_cat,on=['item_category_id'],how='left'),x_train_list))\n",
    "x_test_list = list(map(lambda x: x.merge(df_item_cat,on=['item_category_id'],how='left'),x_test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some item_types have unreliable price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_types_with_unreliable_pricing = ['Игры PC ', 'Кино ', 'Игры ', 'Подарки ', 'Служебные']\n",
    "def remove_unreliable_pricing(df):\n",
    "    df.loc[df.item_type.isin(item_types_with_unreliable_pricing),'prop_median_item_price'] = 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_list = list(map(remove_unreliable_pricing,x_train_list))\n",
    "x_test_list = list(map(remove_unreliable_pricing,x_test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop name vars\n",
    "def drop_names(df):\n",
    "    return df.drop(['item_name','shop_name','item_category_name'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_list = list(map(drop_names,x_train_list))\n",
    "x_test_list = list(map(drop_names,x_test_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Historical Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train , y_train in zip(x_train_list,y_train_list):\n",
    "    train['item_cnt_month'] = y_train.copy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(df,variables,newvarname):\n",
    "    var_sales = train.groupby(variables,as_index=False).item_cnt_month.sum()\n",
    "    var_sales.columns = variables + [newvarname]\n",
    "    df = df.merge(var_sales,how='left')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cat_vars_2(train):\n",
    "    train = merge_data(train,['date_block_num','item_id'],'sum_item_sales_back_0')\n",
    "    train = merge_data(train,['date_block_num','shop_id'],'sum_shop_sales_back_0')\n",
    "    train = merge_data(train,['date_block_num','shop_id','item_id'],'item_cnt_month_back_0')\n",
    "    train = merge_data(train,['date_block_num','item_category_id'],'sum_item_cat_sales_back_0')\n",
    "    train = merge_data(train,['date_block_num','shop_id','item_category_id'],'sum_item_cat_shop_sales_back_0')\n",
    "    train = merge_data(train,['date_block_num','city'],'sum_city_back_0')\n",
    "    train = merge_data(train,['date_block_num','item_type'],'sum_itemtype_back_0')\n",
    "    train = merge_data(train,['date_block_num','item_info'],'sum_iteminfo_back_0')\n",
    "    train = merge_data(train,['date_block_num','city','item_id'],'sum_city_item_back_0')\n",
    "    train = merge_data(train,['date_block_num','city','item_category_id'],'sum_city_item_cat_back_0')\n",
    "    train = merge_data(train,['date_block_num','item_type','shop_id'],'sum_itemtype_shop_back_0')\n",
    "    train = merge_data(train,['date_block_num','item_type','city'],'sum_itemtype_city_back_0')\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_list = list(map(create_cat_vars_2,x_train_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate 0 Entries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to include entries where 0 sales were made for item/shop pairs in a month.\n",
    "So this doesnt get out of hand, gonna focus only on all possible item/shop pairs based on sales in that month, \n",
    "this is what the coursera course did, see outside/Programming_assignment_week_4.ipynb for more info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gen_zeros(train):\n",
    "    months = range(train.date_block_num.min(),train.date_block_num.max()+1)\n",
    "    to_pandas=[]\n",
    "    print('Computing for month:',end=' ')\n",
    "    for month in months:\n",
    "        print(month,end=', ')\n",
    "        subtrain = train[train.date_block_num==month].copy()\n",
    "        all_shops = subtrain.shop_id.unique()\n",
    "        all_items = subtrain.item_id.unique()\n",
    "        pairs = product(all_shops,all_items)\n",
    "        to_pandas.append([(month,x[0],x[1]) for x in pairs])\n",
    "    \n",
    "    train_filled = pd.DataFrame(np.vstack(to_pandas),columns=['date_block_num','shop_id','item_id'])\n",
    "    \n",
    "    #need to fill name vars\n",
    "    #name_df = train[['shop_id','item_id','item_category_id','item_name','shop_name','item_category_name']]\n",
    "    #train_filled = train_filled.merge(name_df)\n",
    "    \n",
    "    print(train_filled.shape)\n",
    "    train_filled = train_filled.merge(train,how='left',on=['date_block_num','shop_id','item_id'])\n",
    "    \n",
    "    print(train_filled.shape)\n",
    "    #Re-merge item_category_id and resort columns, this is computationally inefficient but saves time coding\n",
    "    train_filled = train_filled.drop('item_category_id',axis=1).merge(items_data[['item_id','item_category_id']],on='item_id',how='left')\n",
    "    cols = train_filled.columns.tolist()\n",
    "    col_reseq = cols[0:3] + [cols[-1]] + cols[3:-1]\n",
    "    train_filled = train_filled[col_reseq]\n",
    "    #entries where item_cnt_month is supposed to be 0 are now created as NA\n",
    "    train_filled.fillna(0,inplace=True)\n",
    "    \n",
    "    return train_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing for month: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, (10675678, 3)\n",
      "(10675678, 23)\n",
      "Computing for month: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, (10913850, 3)\n",
      "(10913850, 23)\n"
     ]
    }
   ],
   "source": [
    "x_train_list = list(map(gen_zeros,x_train_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Lag Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "newvars = ['sum_item_sales_back_0','sum_shop_sales_back_0','item_cnt_month_back_0','sum_item_cat_sales_back_0','sum_item_cat_shop_sales_back_0','sum_city_back_0','sum_itemtype_back_0','sum_iteminfo_back_0','sum_city_item_back_0','sum_city_item_cat_back_0','sum_itemtype_shop_back_0','sum_itemtype_city_back_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make this more efficient?\n",
    "def create_lag_train(train_filled):\n",
    "    #gonna iterively copy a subset of the data, rename the date block and aome other cols then merge it back in dateback_gen = range(1,13)\n",
    "    dateback_gen = range(1,DATEBACK_DIST+1)\n",
    "    lag_train_filled = downcast(train_filled)\n",
    "    del train_filled\n",
    "    print('dateback=',end=' ')\n",
    "    gc.collect()\n",
    "    for dateback in dateback_gen: \n",
    "        #this line inneficient?\n",
    "#       to_shift = lag_train_filled[['date_block_num','shop_id','item_id','sum_item_sales_back_0','sum_shop_sales_back_0','item_cnt_month_back_0','sum_item_cat_sales_back_0','sum_item_cat_shop_sales_back_0']].copy()\n",
    "        to_shift = lag_train_filled[['date_block_num','shop_id','item_id']+newvars].copy()\n",
    "        to_shift['date_block_num'] = to_shift.date_block_num + dateback\n",
    "        newcols = ['date_block_num','shop_id','item_id'] + [x[0:-1]+str(dateback) for x in newvars]\n",
    "        to_shift.columns = newcols\n",
    "        #print(newcols)\n",
    "        print(dateback,end=', ')\n",
    "        lag_train_filled = lag_train_filled.merge(to_shift,on=['date_block_num','shop_id','item_id'],how='left').fillna(0)\n",
    "        del to_shift\n",
    "        gc.collect()\n",
    "    \n",
    "    #remove first DATEBACK_DIST many months\n",
    "    lag_train_filled = lag_train_filled[lag_train_filled.date_block_num>=DATEBACK_DIST]\n",
    "    return lag_train_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dateback= 1, 2, dateback= 1, 2, Runtime: 3.1014426271120707\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "x_train_list = list(map(create_lag_train,x_train_list))\n",
    "end = time.time()\n",
    "print('Runtime: '+str((end-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9934775, 47)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1019689122835796"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(time.time() - start)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6425094, 47)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should be (6425094, 167)\n",
    "x_train_list[1][x_train_list[1].date_block_num>=12].shape\n",
    "#This should have 6425094 to mimmick the course script (except this is using all of the shops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_test(tup):\n",
    "    train = tup[0]\n",
    "    test = tup[1]\n",
    "    test = downcast(test)\n",
    "    ref_date_block = test.date_block_num.mean()\n",
    "    dateback_gen = range(1,DATEBACK_DIST+1)\n",
    "    print('Getting information from month:')\n",
    "    for dateback in dateback_gen:\n",
    "        print(str(ref_date_block - dateback),end=', ')\n",
    "        #hist_data = train[train.date_block_num==ref_date_block - dateback][['date_block_num','shop_id','item_id','sum_item_sales_back_0','sum_shop_sales_back_0','item_cnt_month_back_0','sum_item_cat_sales_back_0','sum_item_cat_shop_sales_back_0']]\n",
    "        hist_data = train[train.date_block_num==ref_date_block - dateback][['date_block_num','shop_id','item_id'] + newvars]\n",
    "        hist_data.date_block_num = ref_date_block\n",
    "        #hist_data.columns = ['date_block_num','shop_id','item_id','sum_item_sales_back_'+str(dateback),'sum_shop_sales_back_'+str(dateback),'item_cnt_month_back_'+str(dateback),'sum_item_cat_sales_back_'+str(dateback),'sum_item_cat_shop_sales_back_'+str(dateback)]\n",
    "        newcols = ['date_block_num','shop_id','item_id'] + [x[0:-1]+str(dateback) for x in newvars]\n",
    "        hist_data.columns = newcols\n",
    "        test = test.merge(hist_data,how='left').fillna(0)\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting information from month:\n",
      "32.0, 31.0, Getting information from month:\n",
      "33.0, 32.0, "
     ]
    }
   ],
   "source": [
    "x_test_list = list(map(create_lag_test,zip(x_train_list,x_test_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_train(x_train):\n",
    "    return x_train.item_cnt_month.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_list = list(map(get_y_train,x_train_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_train(train):\n",
    "    sum_item_sales_back_0 = train.sum_item_sales_back_0\n",
    "    sum_shop_sales_back_0 = train.sum_shop_sales_back_0\n",
    "    item_cnt_month_back_0 = train.item_cnt_month_back_0\n",
    "\n",
    "    #train = train.drop(['sum_item_sales_back_0','sum_shop_sales_back_0','item_cnt_month_back_0','item_cnt_month','sum_item_cat_sales_back_0','sum_item_cat_shop_sales_back_0'],axis=1)\n",
    "    train = train.drop(['item_cnt_month'] + newvars,axis=1)\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_list = list(map(clear_train,x_train_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realign y_train with x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_train_indicies(series):\n",
    "    return series.reset_index().drop('index',axis=1).item_cnt_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_list = list(map(reset_train_indicies,y_train_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('saving...')\n",
    "pickle.dump(x_train_list,open('../gen_data/x_train--features1.1.ipynb--.pickle','wb'))\n",
    "print('saving...')\n",
    "pickle.dump(x_test_list,open('../gen_data/x_test--features1.1.ipynb--.pickle','wb'))\n",
    "print('saving...')\n",
    "pickle.dump(y_train_list,open('../gen_data/y_train--features1.1.ipynb--.pickle','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
