{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a DF relating item category name and shop name to shop ids and item ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_ENCODING_NPERM = 8\n",
    "KFOLD_K = 5\n",
    "KFOLD_ALPHA=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import gc\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_item_cat = pd.read_csv('../gen_data/train2.csv')\n",
    "\n",
    "y_train_list = pickle.load(open('../gen_data/y_train--features1.ipynb--.pickle','rb'))\n",
    "x_train_list = pickle.load(open('../gen_data/x_train--features3.ipynb--.pickle','rb'))\n",
    "x_test_list = pickle.load(open('../gen_data/x_test--features3.ipynb--.pickle','rb'))\n",
    "\n",
    "sales = pd.read_csv('../original_data/sales_train.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city = df_item_cat.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate new item variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with sales data, figure out when each item was sold, \n",
    "    #merge this with train/test then transform it to 1/0\n",
    "#fillna with new item?\n",
    "def new_item_var(df):\n",
    "    sales2 = sales.copy()\n",
    "    first_sale_date = sales2.groupby('item_id').date_block_num.min()\n",
    "    sales2['first_sale_date_block'] = sales2.item_id.map(first_sale_date)\n",
    "    sales2 = sales2[['item_id','first_sale_date_block']]\n",
    "    sales2 = sales2.drop_duplicates()\n",
    "    df = df.merge(sales2,on='item_id',how='left')\n",
    "    \n",
    "    #handle nans for test data (which sales2 wont pick up)\n",
    "    df.first_sale_date_block.fillna(999,inplace=True)    \n",
    "    df['new_item'] = (df.first_sale_date_block >= df.date_block_num)*1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_list = list(map(new_item_var,x_train_list))\n",
    "x_test_list = list(map(new_item_var,x_test_list))\n",
    "del sales\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse City data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cities_encoder = preprocessing.LabelEncoder()\n",
    "#cities_series = pd.Series([re.search('(.*?) ',n).group() for n in df.shop_name],index=df.index)\n",
    "#cities_encoder.fit(cities_series)\n",
    "#df['city'] = cities_encoder.transform(cities_series)\n",
    "#del cities_series\n",
    "\n",
    "cities_series = pd.Series([re.search('(.*?) ',n).group() for n in df_city.shop_name],index=df_city.index)\n",
    "df_city['city'] = cities_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cities_series.isna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse item category data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#item_type_encoder = preprocessing.LabelEncoder()\n",
    "#item_type_series = df.item_category_name.map(lambda x: re.split('-',x)[0])\n",
    "#item_type_encoder.fit(item_type_series)\n",
    "#df['item_type'] = item_type_encoder.transform(item_type_series)\n",
    "#del item_type_series\n",
    "\n",
    "item_type_series = df_item_cat.item_category_name.map(lambda x: re.split('-',x)[0])\n",
    "df_item_cat['item_type'] = item_type_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse more item_category data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#item_info_encoder = preprocessing.LabelEncoder()\n",
    "#item_info_series = df.item_category_name.map(lambda x: '-'.join(re.split('-',x)[1:]) if len(re.split('-',x))>1 else 'NAN')\n",
    "#item_info_encoder.fit(item_info_series)\n",
    "#df['item_info'] = item_info_encoder.transform(item_info_series)\n",
    "#del item_info_series\n",
    "\n",
    "item_info_series = df_item_cat.item_category_name.map(lambda x: '-'.join(re.split('-',x)[1:]) if len(re.split('-',x))>1 else 'NAN')\n",
    "df_item_cat['item_info'] = item_info_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select relevant columns in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city = df_city[['shop_id','city']].drop_duplicates()\n",
    "df_item_cat = df_item_cat[['item_category_id','item_type','item_info']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to downcast data types to 32 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downcast(df):\n",
    "    float_cols = [col for col in df if df[col].dtype=='float64']\n",
    "    int_cols = [col for col in df if df[col].dtype=='int64']\n",
    "\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols] = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city = downcast(df_city)\n",
    "df_item_cat = downcast(df_item_cat)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge data above into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_list_saved = x_train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2474769, 72)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-9b4af11f0121>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train_list_saved\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#d.merge(df,on=['item_category_id'],how='left')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "d = x_train_list_saved[0].sample(frac=0.4)\n",
    "print(d.shape)\n",
    "#print(df.shape)\n",
    "#d.merge(df,on=['item_category_id'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_item_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_list = list(map(lambda x: x.merge(df_item_cat,on=['item_category_id'],how='left'),x_train_list))\n",
    "x_test_list = list(map(lambda x: x.merge(df_item_cat,on=['item_category_id'],how='left'),x_test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_list = list(map(lambda x: x.merge(df_city,on=['shop_id'],how='left'),x_train_list))\n",
    "x_test_list = list(map(lambda x: x.merge(df_city,on=['shop_id'],how='left'),x_test_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0029689838000813685\n"
     ]
    }
   ],
   "source": [
    "tocheck = x_train_list[1]\n",
    "print(np.mean(tocheck.city.isna()))\n",
    "print(np.mean(tocheck.item_type.isna()))\n",
    "print(np.mean(tocheck.item_info.isna()))\n",
    "print(np.mean(tocheck.item_info=='NAN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(x_train_list[1].city.isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-13707fc5aac4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#np.mean(df.city.isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-382acc437744>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train_list_saved\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date_block_num'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'shop_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'item_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'item_category_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#new.head()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnew\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "new = x_train_list_saved[1][['date_block_num','shop_id','item_id','item_category_id']].merge(df,how='left')\n",
    "#new.head()\n",
    "print(np.mean(new.city.isna()))\n",
    "new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newna = new[new.city.isna()]\n",
    "newna.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-336dbf7d092a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnew\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshop_id\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'new' is not defined"
     ]
    }
   ],
   "source": [
    "new[(new.shop_id==2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df doesnt contain every shop item combo so when i merge it is misssing a lot,\n",
    "#need to merge separatl on shop and item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_list_saved[1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_list[1].city.unique()\n",
    "#x_test_list[1].city.unique()\n",
    "np.sum(cities_series=='!Якутск ')\n",
    "df.city.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.city=='!Якутск '].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_category_id</th>\n",
       "      <th>sum_item_sales_back_1</th>\n",
       "      <th>sum_shop_sales_back_1</th>\n",
       "      <th>item_cnt_month_back_1</th>\n",
       "      <th>sum_item_cat_sales_back_1</th>\n",
       "      <th>sum_item_cat_shop_sales_back_1</th>\n",
       "      <th>sum_item_sales_back_2</th>\n",
       "      <th>...</th>\n",
       "      <th>numdays</th>\n",
       "      <th>mean_prevmonth_item_price</th>\n",
       "      <th>sd_prevmonth_item_price</th>\n",
       "      <th>mean_prevmonth_shop_item_price</th>\n",
       "      <th>sd_above_market</th>\n",
       "      <th>first_sale_date_block</th>\n",
       "      <th>new_item</th>\n",
       "      <th>item_type</th>\n",
       "      <th>item_info</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [date_block_num, shop_id, item_id, item_category_id, sum_item_sales_back_1, sum_shop_sales_back_1, item_cnt_month_back_1, sum_item_cat_sales_back_1, sum_item_cat_shop_sales_back_1, sum_item_sales_back_2, sum_shop_sales_back_2, item_cnt_month_back_2, sum_item_cat_sales_back_2, sum_item_cat_shop_sales_back_2, sum_item_sales_back_3, sum_shop_sales_back_3, item_cnt_month_back_3, sum_item_cat_sales_back_3, sum_item_cat_shop_sales_back_3, sum_item_sales_back_4, sum_shop_sales_back_4, item_cnt_month_back_4, sum_item_cat_sales_back_4, sum_item_cat_shop_sales_back_4, sum_item_sales_back_5, sum_shop_sales_back_5, item_cnt_month_back_5, sum_item_cat_sales_back_5, sum_item_cat_shop_sales_back_5, sum_item_sales_back_6, sum_shop_sales_back_6, item_cnt_month_back_6, sum_item_cat_sales_back_6, sum_item_cat_shop_sales_back_6, sum_item_sales_back_7, sum_shop_sales_back_7, item_cnt_month_back_7, sum_item_cat_sales_back_7, sum_item_cat_shop_sales_back_7, sum_item_sales_back_8, sum_shop_sales_back_8, item_cnt_month_back_8, sum_item_cat_sales_back_8, sum_item_cat_shop_sales_back_8, sum_item_sales_back_9, sum_shop_sales_back_9, item_cnt_month_back_9, sum_item_cat_sales_back_9, sum_item_cat_shop_sales_back_9, sum_item_sales_back_10, sum_shop_sales_back_10, item_cnt_month_back_10, sum_item_cat_sales_back_10, sum_item_cat_shop_sales_back_10, sum_item_sales_back_11, sum_shop_sales_back_11, item_cnt_month_back_11, sum_item_cat_sales_back_11, sum_item_cat_shop_sales_back_11, sum_item_sales_back_12, sum_shop_sales_back_12, item_cnt_month_back_12, sum_item_cat_sales_back_12, sum_item_cat_shop_sales_back_12, month, numdays, mean_prevmonth_item_price, sd_prevmonth_item_price, mean_prevmonth_shop_item_price, sd_above_market, first_sale_date_block, new_item, item_type, item_info, city]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 75 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc = x_train_list[1]\n",
    "tc[(tc.shop_id==0)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-a8c59fc22a6b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'item_category_id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'city'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "tc[['item_category_id','city']]\n",
    "np.mean(df.city.isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(tc.city.isna())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#should be 0\n",
    "print(np.sum(tc.city.isna()))\n",
    "#should be 0\n",
    "print(np.sum(tc.item_type.isna()))\n",
    "#i would imagine should be small, sometimes info isnt parsed\n",
    "print(np.mean(tc.item_info=='NAN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(x) for x in df.city.unique()]\n",
    "print('\\n\\n')\n",
    "[print(x) for x in df.item_type.unique()]\n",
    "print('\\n\\n')\n",
    "[print(x) for x in df.item_info.unique()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### it is possible (should check) that all of the !Якутск transactions were early and thus not picked up, should still fix because datebackgen will change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum((df.city=='!Якутск '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc[(tc.shop_id==50)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.item_id==30) & (df.shop_id==2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.item_category_id.unique().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc[tc.city.isna()][['item_id','item_category_id','shop_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target encoding is based on historical information, available for both train and test datasets, as such, we needn't worry about regularisation specifically for target encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_target_encode(x_train,y_train,x_test,cols,k=5,alpha=0):\n",
    "    x_train['target'] = y_train\n",
    "    globmean = x_train.target.mean()\n",
    "#for test data\n",
    "    for col in cols:\n",
    "        agged_sum = x_train.groupby(col).target.sum()\n",
    "        x_test[col+'_encoded'] = x_test[col].map(agged_sum)\n",
    "        x_test[col+'_encoded'].fillna(globmean,inplace=True)\n",
    "            \n",
    "#for train data, need to regularise\n",
    "    #ensure index is reset to avoid breaking later\n",
    "    \n",
    "\n",
    "    x_train.reset_index(drop=True,inplace=True)\n",
    "    x_train[col+'_encoded']=np.nan\n",
    "    for col in cols:\n",
    "        print('Encoding '+col)\n",
    "        folds = model_selection.KFold(n_splits=k,shuffle=True,random_state=0)\n",
    "        for compute_ind , map_ind in folds.split(x_train):\n",
    "            mean = x_train.iloc[compute_ind].groupby(col).target.mean()\n",
    "            count = x_train.iloc[compute_ind].groupby(col).target.count()\n",
    "            encoding = (mean*count + globmean*alpha)/(alpha+count)\n",
    "            x_train.loc[map_ind,col+'_encoded'] = x_train.loc[map_ind,col].map(encoding) \n",
    "            x_train[col+'_encoded'].fillna(globmean,inplace=True)\n",
    "    return x_train.drop('target',axis=1) , x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perm_target_encode(x_train,y_train,x_test,cols,nperms):\n",
    "    x_train['target'] = y_train\n",
    "    print('top: '+str(x_train.shape))\n",
    "#for test data\n",
    "    for col in cols:\n",
    "        agged_sum = x_train.groupby(col).target.sum()\n",
    "        x_test[col+'_encoded'] = x_test[col].map(agged_sum)\n",
    "        x_test[col+'_encoded'].fillna(0,inplace=True)\n",
    "            \n",
    "#for train data, need to regularise\n",
    "    for col in cols:\n",
    "        encoding_df = pd.DataFrame()\n",
    "        for perm_num in range(0,nperms):\n",
    "            print('\\tComputing encoding on permutation '+str(perm_num)+' of '+str(nperms))\n",
    "            #set random state to perm num for reproducibility\n",
    "            #make this faster by just changing index?\n",
    "            perm_x_train = x_train.sample(frac=1,random_state=perm_num)\n",
    "            cumsum   = perm_x_train.groupby(col).target.cumsum() - perm_x_train.target\n",
    "            cumcount = perm_x_train.groupby(col).target.cumcount()\n",
    "            encoding = cumsum/cumcount\n",
    "            \n",
    "            encoding_df['perm_'+str(perm_num)+'_encoding'] = encoding\n",
    "            \n",
    "        encoding_df['colmeans'] = encoding_df.mean(axis=1)    \n",
    "        x_train[col+'_encoded'] = encoding_df.colmeans\n",
    "        x_train[col+'_encoded'].fillna(0,inplace=True)\n",
    "        del encoding_df\n",
    "        gc.collect()\n",
    "    return x_train.drop('target',axis=1) , x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_target_encode = ['shop_id','item_id','item_category_id','month','city','item_type','item_info']\n",
    "to_target_encode=['shop_id']\n",
    "jumbled_list = [kfold_target_encode(x_train,y_train,x_test,to_target_encode,KFOLD_K,alpha=KFOLD_ALPHA) for x_train , y_train, x_test in zip(x_train_list,y_train_list,x_test_list)]\n",
    "x_train_list , x_test_list = list(map(list,zip(*jumbled_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = x_train_list[1]\n",
    "d['target'] = y_train_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[d.shop_id==50].shop_id_encoded.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_list[0]['shop_id_encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.groupby('shop_id').target.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(x_train_list,open('../gen_data/x_train--features4.ipynb--.pickle','wb'))\n",
    "pickle.dump(x_test_list,open('../gen_data/x_test--features4.ipynb--.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_list[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to_target_encode = ['shop_id','item_id','item_category_id','month','city','item_type','item_info']\n",
    "#jumbled_list = [perm_target_encode(x_train,y_train,x_test,to_target_encode,TARGET_ENCODING_NPERM) for x_train , y_train, x_test in zip(x_train_list,y_train_list,x_test_list)]\n",
    "#x_train_list , x_test_list = list(map(list,zip(*jumbled_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to add bool: item never sold before\n",
    "                    #shop never sold before\n",
    "# and int: num item sold\n",
    "# num shop sold\n",
    "# FOR EACH MONTH\n",
    "\n",
    "#this is target encoding filling nans with 0 except missing early data\n",
    "\n",
    "#doing this early in pipeline would be easy\n",
    "\n",
    "#going back for all time might be a bad idea, the nature of the data would change thru time, (summingup)\n",
    "#instaed just use dateback range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create variables to quantify how much we know about a data point\n",
    "#For each shop_id and item_id create sum of recent history(total sales for dateback_range)\n",
    "#create bool if 0 indicating new shop or item\n",
    "def generate_recent_history_summary(df):\n",
    "    df_items = df.filter(regex='sum_item_sales_back_\\d+$')\n",
    "    df['historical_item_sales'] = df_items.sum(axis=1)\n",
    "    df['new_item'] = (df.historical_item_sales<0.5)*1\n",
    "    \n",
    "    df_shops = df.filter(regex='sum_shop_sales_back_\\d+$')\n",
    "    df['historical_shop_sales'] = df_shops.sum(axis=1)\n",
    "    df['new_shop'] = (df.historical_shop_sales<0.5)*1\n",
    "    \n",
    "    df_item_cats = df.filter(regex='sum_item_cat_sales_back_\\d+$')\n",
    "    df['historical_item_cat_sales'] = df_item_cats.sum(axis=1)\n",
    "    df['new_item_cat'] = (df.historical_item_cat_sales<0.5)*1\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_list = list(map(generate_recent_history_summary,x_train_list))\n",
    "x_test_list = list(map(generate_recent_history_summary,x_test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_list[0][['item_id','historical_item_sales']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = x_train_list[0][['date_block_num','item_id','item_id_encoded','historical_item_sales']].copy()\n",
    "df.columns = ['date_block_num','item_id','ORIGINAL_item_id_encoded','historical_item_sales']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#work on this\n",
    "#this isnt working properly, drop it, I have almost the same information from elsewhere\n",
    "def old_target_encode(df,target,cols,fillna=0):\n",
    "    maxmonth = df.date_block_num.max()\n",
    "    print(maxmonth)\n",
    "    df['target'] = target\n",
    "    for col in cols:\n",
    "        print('Encoding variable: '+col)\n",
    "        #dont use current data, that way we can do this for both train and test sets\n",
    "        agged_targ = df[df.date_block_num<maxmonth].groupby(col).target.sum()\n",
    "        agged_targ = df[(df.date_block_num<maxmonth) & (df.date_block_num>=(maxmonth-12))].groupby(col).target.sum()\n",
    "        #agged_targ = df[df.date_block_num>20].groupby(col).target.sum()\n",
    "        \n",
    "        \n",
    "        \n",
    "        df[col+'_encoded'] = df[col].map(agged_targ)\n",
    "        if fillna=='mean':\n",
    "            df[col].fillna(df(col).mean(),inplace=True)\n",
    "        elif fillna=='median':\n",
    "            df[col].fillna(df(col).median(),inplace=True)\n",
    "        else:\n",
    "            df[col].fillna(0,inplace=True)\n",
    "    return df.drop('target',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Im counting back the months from max month, but historical is rolling for each date, historical is bad becuase it double counts, or does it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#targs = y_train_list[0]\n",
    "#target_encode(df,targs,['item_id'])\n",
    "#g = df[(df.date_block_num<32) & (df.date_block_num>=20)].groupby('item_id').target.sum()\n",
    "#df['item_id_encoded_manual'] = df.item_id.map(g)\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "21 ... 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = x_train_list[1]\n",
    "targs = y_train_list[1]\n",
    "df['target'] = targs\n",
    "df_small = df[['item_id','item_id_encoded','target','historical_item_sales']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = df.groupby('item_id').target.sum()\n",
    "g.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small['item_id_encoded_manual'] = df.item_id.map(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.item_id==32)].target.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[['item_id','item_idencoded']].sort_values('item_id').head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df100 = df.sample(100)\n",
    "df100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
