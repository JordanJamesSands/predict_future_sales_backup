{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a DF relating item category name and shop name to shop ids and item ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import gc\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../gen_data/train2.csv')\n",
    "\n",
    "y_train_list = pickle.load(open('../gen_data/y_train--features1.ipynb--.pickle','rb'))\n",
    "x_train_list = pickle.load(open('../gen_data/x_train--features3.ipynb--.pickle','rb'))\n",
    "x_test_list = pickle.load(open('../gen_data/x_test--features3.ipynb--.pickle','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse City data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_encoder = preprocessing.LabelEncoder()\n",
    "cities_series = pd.Series([re.search('(.*?) ',n).group() for n in df.shop_name],index=df.index)\n",
    "cities_encoder.fit(cities_series)\n",
    "df['city'] = cities_encoder.transform(cities_series)\n",
    "del cities_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse item category data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_type_encoder = preprocessing.LabelEncoder()\n",
    "item_type_series = df.item_category_name.map(lambda x: re.split('-',x)[0])\n",
    "item_type_encoder.fit(item_type_series)\n",
    "df['item_type'] = item_type_encoder.transform(item_type_series)\n",
    "del item_type_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse more item_category data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_info_encoder = preprocessing.LabelEncoder()\n",
    "item_info_series = df.item_category_name.map(lambda x: '-'.join(re.split('-',x)[1:]) if len(re.split('-',x))>1 else 'NAN')\n",
    "item_info_encoder.fit(item_info_series)\n",
    "df['item_info'] = item_info_encoder.transform(item_info_series)\n",
    "del item_info_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select relevant columns in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['shop_id','item_id','item_category_id','city','item_type','item_info']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to downcast data types to 32 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downcast(df):\n",
    "    float_cols = [col for col in df if df[col].dtype=='float64']\n",
    "    int_cols = [col for col in df if df[col].dtype=='int64']\n",
    "\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols] = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = downcast(df)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge data above into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_list = list(map(lambda x: x.merge(df,on=['item_id','item_category_id','shop_id'],how='left'),x_train_list))\n",
    "x_test_list = list(map(lambda x: x.merge(df,on=['item_id','item_category_id','shop_id'],how='left'),x_test_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target encoding is based on historical information, available for both train and test datasets, as such, we needn't worry about regularisation specifically for target encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encode(df,target,cols,fillna=0):\n",
    "    df['target'] = target\n",
    "    for col in cols:\n",
    "        print('Encoding variable: '+col)\n",
    "        agged_targ = df.groupby(col).target.sum()\n",
    "        #agged_targ = df[df.date_block_num>20].groupby(col).target.sum()\n",
    "        df[col] = df[col].map(agged_targ)\n",
    "        if fillna=='mean':\n",
    "            df[col].fillna(df(col).mean(),inplace=True)\n",
    "        elif fillna=='median':\n",
    "            df[col].fillna(df(col).median(),inplace=True)\n",
    "        else:\n",
    "            df[col].fillna(0,inplace=True)\n",
    "    return df.drop('target',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_list = [target_encode(train,y_train_list[0],['shop_id','item_id','item_category_id','month','city','item_type','item_info'],fillna=0) for train in x_train_list]\n",
    "x_test_list = [target_encode(test,y_train_list[0],['shop_id','item_id','item_category_id','month','city','item_type','item_info'],fillna=0) for test in x_test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to add bool: item never sold before\n",
    "                    #shop never sold before\n",
    "# and int: num item sold\n",
    "# num shop sold\n",
    "# FOR EACH MONTH\n",
    "\n",
    "#this is target encoding filling nans with 0 except missing early data\n",
    "\n",
    "#doing this early in pipeline would be easy\n",
    "\n",
    "#going back for all time might be a bad idea, the nature of the data would change thru time, (summingup)\n",
    "#instaed just use dateback range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(n) for n in x_train_list[0].columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfout = x_train_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_items = dfout.filter(regex='sum_item_sales_back_\\d+$')\n",
    "df_items.sum(axis=1)\n",
    "dfout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create variables to quantify how much we know about a data point\n",
    "#For each shop_id and item_id create sum of recent history(total sales for dateback_range)\n",
    "#create bool if 0 indicating new shop or item\n",
    "def generate_recent_history_summary(df):\n",
    "    df_items = df.filter(regex='sum_item_sales_back_\\d+$')\n",
    "    df['historical_item_sales'] = df_items.sum(axis=1)\n",
    "    df['new_item'] = (df.historical_item_sales<0.5)*1\n",
    "    \n",
    "    df_shops = df.filter(regex='sum_shop_sales_back_\\d+$')\n",
    "    df['historical_shop_sales'] = df_shops.sum(axis=1)\n",
    "    df['new_shop'] = (df.historical_shop_sales<0.5)*1\n",
    "    \n",
    "    df_item_cats = df.filter(regex='sum_item_cat_sales_back_\\d+$')\n",
    "    df['historical_item_cat_sales'] = df_item_cats.sum(axis=1)\n",
    "    df['new_item_cat'] = (df.historical_item_cat_sales<0.5)*1\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_list = list(map(generate_recent_history_summary,x_train_list))\n",
    "x_test_list = list(map(generate_recent_history_summary,x_test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_list[0][['item_id','historical_item_sales']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(x_train_list,open('../gen_data/x_train--strings.ipynb--.pickle','wb'))\n",
    "pickle.dump(x_test_list,open('../gen_data/x_test--strings.ipynb--.pickle','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
