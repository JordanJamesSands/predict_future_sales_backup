{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a DF relating item category name and shop name to shop ids and item ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_ENCODING_NPERM = 8\n",
    "KFOLD_K = 5\n",
    "#KFOLD_ALPHAS=[1000,5000,10000]\n",
    "#KFOLD_ALPHAS = [0,50,100]\n",
    "#KFOLD_ALPHAS = [0,50,100,200,500,1000,2000]\n",
    "KFOLD_ALPHAS = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[0]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(KFOLD_ALPHAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import gc\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_item_cat = pd.read_csv('../gen_data/train2.csv')\n",
    "y_train_list = pickle.load(open('../gen_data/y_train--features1.ipynb--.pickle','rb'))\n",
    "x_train_list = pickle.load(open('../gen_data/x_train--features3.ipynb--.pickle','rb'))\n",
    "x_test_list = pickle.load(open('../gen_data/x_test--features3.ipynb--.pickle','rb'))\n",
    "sales = pd.read_csv('../gen_data/train1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city = df_item_cat.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate new item variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with sales data, figure out when each item was sold, \n",
    "    #merge this with train/test then transform it to 1/0\n",
    "#fillna with new item?\n",
    "def new_item_var(df):\n",
    "    sales2 = sales.copy()\n",
    "    first_sale_date = sales2.groupby('item_id').date_block_num.min()\n",
    "    sales2['first_sale_date_block'] = sales2.item_id.map(first_sale_date)\n",
    "    sales2 = sales2[['item_id','first_sale_date_block']]\n",
    "    sales2 = sales2.drop_duplicates()\n",
    "    df = df.merge(sales2,on='item_id',how='left')\n",
    "    \n",
    "    #handle nans for test data (which sales2 wont pick up)\n",
    "    df.first_sale_date_block.fillna(999,inplace=True)    \n",
    "    df['new_item'] = (df.first_sale_date_block >= df.date_block_num)*1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_list = list(map(new_item_var,x_train_list))\n",
    "x_test_list = list(map(new_item_var,x_test_list))\n",
    "del sales\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to downcast data types to 32 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downcast(df):\n",
    "    float_cols = [col for col in df if df[col].dtype=='float64']\n",
    "    int_cols = [col for col in df if df[col].dtype=='int64']\n",
    "\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols] = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse City data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_series = pd.Series([re.search('(.*?) ',n).group() for n in df_city.shop_name],index=df_city.index)\n",
    "df_city['city'] = cities_series\n",
    "df_city = df_city[['shop_id','city']].drop_duplicates()\n",
    "df_city = downcast(df_city)\n",
    "\n",
    "x_train_list = list(map(lambda x: x.merge(df_city,on=['shop_id'],how='left'),x_train_list))\n",
    "x_test_list = list(map(lambda x: x.merge(df_city,on=['shop_id'],how='left'),x_test_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse item category data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_type_series = df_item_cat.item_category_name.map(lambda x: re.split('-',x)[0])\n",
    "df_item_cat['item_type'] = item_type_series\n",
    "\n",
    "item_info_series = df_item_cat.item_category_name.map(lambda x: '-'.join(re.split('-',x)[1:]) if len(re.split('-',x))>1 else 'NAN')\n",
    "df_item_cat['item_info'] = item_info_series\n",
    "\n",
    "df_item_cat = df_item_cat[['item_category_id','item_type','item_info']].drop_duplicates()\n",
    "df_item_cat = downcast(df_item_cat)\n",
    "\n",
    "x_train_list = list(map(lambda x: x.merge(df_item_cat,on=['item_category_id'],how='left'),x_train_list))\n",
    "x_test_list = list(map(lambda x: x.merge(df_item_cat,on=['item_category_id'],how='left'),x_test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some item_types have unreliable price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_types_with_unreliable_pricing = ['Игры PC ', 'Кино ', 'Игры ', 'Подарки ', 'Служебные']\n",
    "def remove_unreliable_pricing(df):\n",
    "    df.loc[df.item_type.isin(item_types_with_unreliable_pricing),'prop_median_item_price'] = 1\n",
    "    #df.loc[df.item_type.isin(item_types_with_unreliable_pricing),'median_prevmonth_item_price']\n",
    "    #df.loc[df.item_type.isin(item_types_with_unreliable_pricing),'median_prevmonth_shop_item_price']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_list = list(map(remove_unreliable_pricing,x_train_list))\n",
    "x_test_list = list(map(remove_unreliable_pricing,x_test_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### it is possible (should check) that all of the !Якутск transactions were early and thus not picked up, should still fix because datebackgen will change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target encoding is based on historical information, available for both train and test datasets, as such, we needn't worry about regularisation specifically for target encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_target_encode(x_train,y_train,x_test,cols,k=5,alpha=0):\n",
    "    x_train['target'] = y_train\n",
    "    globmean = x_train.target.mean()\n",
    "#for test data\n",
    "    for col in cols:\n",
    "        agged_mean = x_train.groupby(col).target.mean()\n",
    "        x_test[col+'_encoded_alpha_'+str(alpha)] = x_test[col].map(agged_mean)\n",
    "        x_test[col+'_encoded_alpha_'+str(alpha)].fillna(globmean,inplace=True)\n",
    "            \n",
    "#for train data, need to regularise\n",
    "    #ensure index is reset to avoid breaking later\n",
    "    \n",
    "\n",
    "    x_train.reset_index(drop=True,inplace=True)\n",
    "    x_train[col+'_encoded_alpha_'+str(alpha)]=np.nan\n",
    "    for col in cols:\n",
    "        print('Encoding '+col)\n",
    "        folds = model_selection.KFold(n_splits=k,shuffle=True,random_state=0)\n",
    "        for compute_ind , map_ind in folds.split(x_train):\n",
    "            mean = x_train.iloc[compute_ind].groupby(col).target.mean()\n",
    "            count = x_train.iloc[compute_ind].groupby(col).target.count()\n",
    "            encoding = (mean*count + globmean*alpha)/(alpha+count)\n",
    "            x_train.loc[map_ind,col+'_encoded_alpha_'+str(alpha)] = x_train.loc[map_ind,col].map(encoding) \n",
    "            x_train[col+'_encoded_alpha_'+str(alpha)].fillna(globmean,inplace=True)\n",
    "    return x_train.drop('target',axis=1) , x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_list[0] = y_train_list[0][x_train_list[0].date_block_num>=25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(y_train_list[0],open('../gen_data/saved_y_train_list[0].pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_target_encode(x_train,y_train,x_test,cols,k=5,alpha=0):\n",
    "    x_train['target'] = y_train\n",
    "    \n",
    "    #HACKS\n",
    "    x_train = x_train[x_train.date_block_num>=25].copy()\n",
    "    #y_train = y_train[x_train.date_block_num>=30].copy()\n",
    "    ###\n",
    "    \n",
    "    globmean = x_train.target.mean()\n",
    "#for test data\n",
    "    for col in cols:\n",
    "        agged_mean = x_train.groupby(col).target.mean()\n",
    "        x_test[col+'_encoded_alpha_'+str(alpha)] = x_test[col].map(agged_mean)\n",
    "        x_test[col+'_encoded_alpha_'+str(alpha)].fillna(globmean,inplace=True)\n",
    "            \n",
    "#for train data, need to regularise\n",
    "    #ensure index is reset to avoid breaking later\n",
    "    \n",
    "\n",
    "    x_train.reset_index(drop=True,inplace=True)\n",
    "    x_train[col+'_encoded_alpha_'+str(alpha)]=np.nan\n",
    "    for col in cols:\n",
    "        print('Encoding '+col)\n",
    "        folds = model_selection.KFold(n_splits=k,shuffle=True,random_state=0)\n",
    "        for compute_ind , map_ind in folds.split(x_train):\n",
    "            mean = x_train.iloc[compute_ind].groupby(col).target.mean()\n",
    "            count = x_train.iloc[compute_ind].groupby(col).target.count()\n",
    "            encoding = (mean*count + globmean*alpha)/(alpha+count)\n",
    "            x_train.loc[map_ind,col+'_encoded_alpha_'+str(alpha)] = x_train.loc[map_ind,col].map(encoding) \n",
    "            x_train[col+'_encoded_alpha_'+str(alpha)].fillna(globmean,inplace=True)\n",
    "    return x_train.drop('target',axis=1) , x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Parallelise this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha=0\n",
      "Encoding shop_id\n",
      "Encoding item_id\n",
      "Encoding shop_id\n",
      "Encoding item_id\n"
     ]
    }
   ],
   "source": [
    "#to_target_encode = ['shop_id','item_id','item_category_id','month','city','item_type','item_info']\n",
    "to_target_encode = ['shop_id','item_id']\n",
    "for alpha in KFOLD_ALPHAS:\n",
    "    print('alpha='+str(alpha))\n",
    "    jumbled_list = [kfold_target_encode(x_train,y_train,x_test,to_target_encode,KFOLD_K,alpha=alpha) for x_train , y_train, x_test in zip(x_train_list,y_train_list,x_test_list)]\n",
    "    x_train_list , x_test_list = list(map(list,zip(*jumbled_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train_list[0][y_train_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train_list[0][x_train_list[0].date_block_num>=30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(x_train_list,open('../gen_data/x_train_'+str(KFOLD_ALPHAS)+'_--features4.ipynb--.pickle','wb'))\n",
    "pickle.dump(x_test_list,open('../gen_data/x_test_'+str(KFOLD_ALPHAS)+'_--features4.ipynb--.pickle','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
